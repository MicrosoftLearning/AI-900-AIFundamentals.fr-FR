{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reconnaissance optique de caractères\r\n",
        "\r\n",
        "![Un robot lisant un journal](./images/ocr.jpg)\r\n",
        "\r\n",
        "Un défi courant en matière de vision par ordinateur consiste à détecter et à interpréter du texte dans une image. Ce type de traitement est souvent appelé *reconnaissance optique de caractères* (OCR).\r\n",
        "\r\n",
        "## Utiliser le service Vision par ordinateur pour lire du texte dans une image\r\n",
        "\r\n",
        "Le service cognitif **Vision par ordinateur** prend en charge les tâches d’OCR, notamment :\r\n",
        "\r\n",
        "- Une API **OCR** que vous pouvez utiliser pour lire du texte dans plusieurs langues. Cette API peut être utilisée de manière synchrone et fonctionne bien lorsque vous devez détecter et lire une petite quantité de texte dans une image.\r\n",
        "- Une API **Lecture** optimisée pour les documents plus volumineux. Cette API est utilisée de manière asynchrone et peut être utilisée pour les textes imprimés et manuscrits.\r\n",
        "\r\n",
        "Vous pouvez utiliser ce service en créant soit une ressource **Vision par ordinateur** soit une ressource **Cognitive Services**.\r\n",
        "\r\n",
        "Si vous ne l’avez pas encore fait, créez une ressource **Cognitive Services** dans votre abonnement Azure.\r\n",
        "\r\n",
        "> **Remarque** : Si vous disposez déjà d’une ressource Cognitive Services, il suffit d’ouvrir sa page **Démarrage rapide** dans le portail Azure et de copier sa clé et son point de terminaison dans la cellule ci-dessous. Sinon, suivez les étapes ci-dessous pour en créer une.\r\n",
        "\r\n",
        "1. Dans un autre onglet du navigateur, ouvrez le portail Azure à l’adresse https://portal.azure.com, en vous connectant avec votre compte Microsoft.\r\n",
        "\r\n",
        "2. Cliquez sur le bouton **&#65291; Créer une ressource**, recherchez *Cognitive Services*, et créez une ressource **Cognitive Services** avec les paramètres suivants :\r\n",
        "    - **Abonnement** : *Votre abonnement Azure*.\r\n",
        "    - **Groupe de ressources** : *Sélectionnez ou créez un groupe de ressources portant un nom unique*.\r\n",
        "    - **Région** : *Choisissez une région disponible* :\r\n",
        "    - **Nom** : *Saisissez un nom unique*.\r\n",
        "    - **Niveau tarifaire** : S0\r\n",
        "    - **Je confirme avoir lu et compris les avis** : Sélectionné.\r\n",
        "3. Attendez la fin du déploiement. Ensuite, accédez à votre ressource Cognitive Services et, sur la page **Aperçu**, cliquez sur le lien permettant de gérer les clés du service. Vous aurez besoin du point de terminaison et des clés pour vous connecter à votre ressource Cognitive Services à partir d’applications clientes.\r\n",
        "\r\n",
        "### Obtenir la clé et le point de terminaison de votre ressource Cognitive Services\r\n",
        "\r\n",
        "Pour utiliser votre ressource Cognitive Services, les applications clientes ont besoin de son point de terminaison et de sa clé d’authentification :\r\n",
        "\r\n",
        "1. Dans le portail Azure, sur la page **Clés et Point de terminaison** de votre ressource Cognitive Services, copiez la **Clé 1** de votre ressource et collez-la dans le code ci-dessous, en remplaçant **YOUR_COG_KEY**.\r\n",
        "2. Copiez le **point de terminaison** de votre ressource et collez-le dans le code ci-dessous, en remplaçant **YOUR_COG_ENDPOINT**.\r\n",
        "3. Exécutez le code dans la cellule ci-dessous en cliquant sur le bouton **Exécuter la cellule** (&#9655;) (à gauche de la cellule)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694246277
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que vous avez configuré la clé et le point de terminaison, vous pouvez utiliser votre ressource de service de vision par ordinateur pour extraire du texte d’une image.\r\n",
        "\r\n",
        "Commençons par l’API **OCR**, qui vous permet d’analyser de manière synchrone une image et de lire le texte qu’elle contient. Dans ce cas, vous disposez d’une image publicitaire pour l’entreprise de vente au détail fictive Northwind Traders qui contient du texte. Exécutez la cellule ci-dessous pour le lire... "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'advert.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Use the Computer Vision service to find text in the image\r\n",
        "read_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n",
        "\n",
        "# Process the text line by line\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Read the words in the line of text\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Open image to display it.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694257280
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le texte trouvé dans l’image est organisé en une structure hiérarchique de régions, de lignes et de mots, et le code les lit pour récupérer les résultats.\r\n",
        "\r\n",
        "Dans les résultats, affichez le texte qui a été lu au-dessus de l’image. \r\n",
        "\r\n",
        "## Afficher les cases de délimitation\r\n",
        "\r\n",
        "Les résultats incluent également les coordonnées de la *case de délimitation* pour les lignes de texte et les mots individuels trouvés dans l’image. Exécutez la cellule ci-dessous pour voir les cases de délimitation des lignes de texte dans l’image publicitaire que vous avez récupérée ci-dessus."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Open image to display it.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "\n",
        "# Process the text line by line\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Show the position of the line of text\n",
        "        l,t,w,h = list(map(int, line.bounding_box.split(',')))\n",
        "        draw.rectangle(((l,t), (l+w, t+h)), outline='magenta', width=5)\n",
        "\n",
        "        # Read the words in the line of text\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Show the image with the text locations highlighted\r\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694266106
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le résultat, la case de délimitation de chaque ligne de texte est représentée par un rectangle sur l’image.\r\n",
        "\r\n",
        "## Utiliser l’API Lire\r\n",
        "\r\n",
        "L’API OCR que vous avez utilisée précédemment fonctionne bien pour les images contenant une petite quantité de texte. Lorsque vous devez lire des textes plus volumineux, tels que des documents numérisés, vous pouvez utiliser l’API **Lire**. Ce processus nécessite plusieurs étapes :\r\n",
        "\r\n",
        "1. Soumettez une image au service Vision par ordinateur pour qu’elle soit lue et analysée de manière asynchrone.\r\n",
        "2. Attendez que l’opération d’analyse se termine.\r\n",
        "3. Récupérez les résultats de l’analyse.\r\n",
        "\r\n",
        "Exécutez la cellule suivante pour utiliser ce processus afin de lire le texte d’une lettre numérisée adressée au directeur d’un magasin Northwind Traders."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'letter.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Submit a request to read printed text in the image and get the operation ID\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Wait for the asynchronous operation to complete\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# If the operation was successfuly, process the text line by line\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Open image and display it.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694312346
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examinez les résultats. Il y a une transcription complète de la lettre, qui consiste principalement en un texte imprimé avec une signature manuscrite. L’image originale de la lettre est affichée sous les résultats de l’OCR (vous devrez peut-être faire défiler l’écran pour la voir).\r\n",
        "\r\n",
        "## Lisez le texte manuscrit\r\n",
        "\r\n",
        "Dans l’exemple précédent, la demande d’analyse de l’image spécifiait un mode de reconnaissance de texte qui optimisait l’opération pour le texte *imprimé*. Notez que malgré cela, la signature manuscrite a été lue.\r\n",
        "\r\n",
        "Cette capacité à lire du texte manuscrit est extrêmement utile. Par exemple, supposons que vous ayez écrit une note contenant une liste de courses, et que vous souhaitiez utiliser une application sur votre téléphone pour lire la note et transcrire le texte qu’elle contient.\r\n",
        "\r\n",
        "Exécutez la cellule ci-dessous pour voir un exemple d’une opération de lecture d’une liste de courses manuscrite."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'note.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Submit a request to read printed text in the image and get the operation ID\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Wait for the asynchronous operation to complete\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# If the operation was successfuly, process the text line by line\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Open image and display it.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694340593
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Informations complémentaires\r\n",
        "\r\n",
        "Pour plus d’informations sur l’utilisation du service Vision par ordinateur pour l’OCR, consultez [la documentation Vision par ordinateur](https://docs.microsoft.com/fr-fr/azure/cognitive-services/computer-vision/concept-recognizing-text)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}