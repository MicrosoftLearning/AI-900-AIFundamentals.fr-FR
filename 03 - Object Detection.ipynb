{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Détection d’objets\r\n",
        "\r\n",
        "*Détection d’objets* est une forme de vision informatique dans laquelle un modèle d’apprentissage automatique est entraîné à classer les instances individuelles d’objets dans une image et à indiquer une *case de délimitation* qui marque son emplacement. Vous pouvez considérer qu’il s’agit d’une progression de la *classification d’images* (dans laquelle le modèle répond à la question « de quoi s’agit-il ? ») vers la construction de solutions où l’on peut demander au modèle « quels sont les objets présents dans cette image, et où sont-ils ? ».\r\n",
        "\r\n",
        "![Un robot identifiant un fruit](./images/object-detection.jpg)\r\n",
        "\r\n",
        "Par exemple, une épicerie pourrait utiliser un modèle de détection d’objets pour mettre en œuvre un système de caisse automatisé qui scanne un tapis roulant à l’aide d’une caméra et peut identifier des articles spécifiques sans avoir besoin de placer chaque article sur le tapis et de les scanner individuellement.\r\n",
        "\r\n",
        "Le service cognitif **Vision personnalisée** de Microsoft Azure offre une solution basée sur le cloud pour créer et publier des modèles de détection d’objets personnalisés.\r\n",
        "\r\n",
        "## Créer une ressource Vision personnalisée\r\n",
        "\r\n",
        "Pour utiliser le service Vision personnalisée, vous avez besoin d’une ressource Azure que vous pouvez utiliser pour entraîner un modèle, et d’une ressource avec laquelle vous pouvez le publier pour que les applications puissent l’utiliser. Vous pouvez utiliser la même ressource pour chacune de ces tâches, ou vous pouvez utiliser des ressources différentes pour chacune d’elles afin d’allouer les coûts séparément, à condition que les deux ressources soient créées dans la même région. La ressource pour l’une ou l’autre (ou les deux) tâches peut être une ressource **Cognitive Services** générale ou une ressource **Vision personnalisée** spécifique. Suivez les instructions suivantes pour créer une nouvelle ressource **Vision personnalisée** (ou vous pouvez utiliser une ressource existante si vous en avez une).\r\n",
        "\r\n",
        "1. Dans un nouvel onglet de navigateur, ouvrez le portail Azure à l’adresse [https://portal.azure.com](https://portal.azure.com), et connectez-vous en utilisant le compte Microsoft associé à votre abonnement Azure.\r\n",
        "2. Sélectionnez le bouton **&#65291; Créer une ressource**, recherchez *Vision personnalisée*, et créez une ressource **Vision personnalisée** avec les paramètres suivants :\r\n",
        "    - **Créer des options** : Les deux\r\n",
        "    - **Abonnement** : *Votre abonnement Azure*\r\n",
        "    - **Groupe de ressources** : *Sélectionnez ou créez un groupe de ressources portant un nom unique*\r\n",
        "    - **Nom** : *Saisissez un nom unique*\r\n",
        "    - **Emplacement de formation** : *Choisissez une région disponible*\r\n",
        "    - **Niveau tarifaire de formation** : F0\r\n",
        "    - **Emplacement de prédiction** : *Le même que l’emplacement de formation*\r\n",
        "    - **Niveau tarifaire de prédiction** : F0\r\n",
        "\r\n",
        "    > **Remarque** : Si vous avez déjà un service de vision personnalisée F0 dans votre abonnement, sélectionnez **S0** pour celui-ci.\r\n",
        "\r\n",
        "3. Attendez que la ressource soit créée.\r\n",
        "\r\n",
        "## Créer un projet Vision personnalisée\r\n",
        "\r\n",
        "Pour entraîner un modèle de détection d’objets, vous devez créer un projet Vision personnalisée basé sur votre ressource de formation. Pour ce faire, vous utiliserez le portail Vision personnalisée.\r\n",
        "\r\n",
        "1. Dans un nouvel onglet de navigateur, ouvrez le portail Vision personnalisée à l’adresse [https://customvision.ai](https://customvision.ai), et connectez-vous en utilisant le compte Microsoft associé à votre abonnement Azure.\r\n",
        "2. Créez un nouveau projet avec les paramètres suivants :\r\n",
        "    - **Nom** : Détection d’épicerie\r\n",
        "    - **Description** : Détection d’objets pour les épiceries.\r\n",
        "    - **Ressource** : *La ressource Vision personnalisée que vous avez créée précédemment*\r\n",
        "    - **Types de projets** : Détection d’objets\r\n",
        "    - **Domaines** : Général\r\n",
        "3. Attendez que le projet soit créée et ouvert dans le navigateur.\r\n",
        "\r\n",
        "## Ajoutez et étiquetez des images\r\n",
        "\r\n",
        "Pour entraîner un modèle de détection d’objets, vous devez télécharger des images contenant les classes que vous souhaitez que le modèle identifie, et les étiqueter pour indiquer les cases de délimitation de chaque instance d’objet.\r\n",
        "\r\n",
        "1. Téléchargez et extrayez les images d’entraînement à partir de https://aka.ms/fruit-objects. Le dossier extrait contient une collection d’images de fruits. **Remarque :** si vous ne pouvez pas accéder aux images de formation, une solution de contournement temporaire consiste à visiter la page https://www.github.com, puis la page https://aka.ms/fruit-objects. \r\n",
        "2. Dans le portail Vision personnalisée [https://customvision.ai](https://customvision.ai), assurez-vous d’utiliser le modèle de détection d’objet project _Grocery Detection_. Sélectionnez ensuite **Add images** (Ajouter des images) et téléchargez toutes les images dans le dossier extrait.\r\n",
        "\r\n",
        "![Téléchargez toutes les images extraites en cliquant sur Add images (Ajouter des images).](./images/fruit-upload.jpg)\r\n",
        "\r\n",
        "3. Une fois les images téléchargées, sélectionnez la première pour l’ouvrir.\r\n",
        "4. Maintenez la souris sur n’importe quel objet de l’image jusqu’à ce qu’une région détectée automatiquement s’affiche comme dans l’image ci-dessous. Sélectionnez ensuite l’objet et, si nécessaire, redimensionnez la région pour l’entourer.\r\n",
        "\r\n",
        "![La région par défaut pour un objet](./images/object-region.jpg)\r\n",
        "\r\n",
        "Vous pouvez aussi simplement faire glisser l’objet pour créer une région.\r\n",
        "\r\n",
        "5. Lorsque la région entoure l’objet, ajoutez une nouvelle étiquette avec le type d’objet approprié (*pomme*, *banane*, ou *orange*) comme illustré ici :\r\n",
        "\r\n",
        "![Un objet étiqueté dans une image](./images/object-tag.jpg)\r\n",
        "\r\n",
        "6. Sélectionnez et étiquetez chaque autre objet de l’image, en redimensionnant les régions et en ajoutant de nouvelles étiquettes si nécessaire.\r\n",
        "\r\n",
        "![Deux objets étiquetés dans une image](./images/object-tags.jpg)\r\n",
        "\r\n",
        "7. Utilisez le lien **>** sur la droite pour passer à l’image suivante et étiqueter ses objets. Puis continuez à parcourir toute la collection d’images, en étiquetant chaque pomme, banane et orange.\r\n",
        "\r\n",
        "8. Lorsque vous avez terminé d’étiqueter la dernière image, fermez l’éditeur **Détails de l’image** et sur la page **Images de formation**, sous **Étiquettes**, sélectionnez **Étiqueté** pour voir toutes vos images étiquetées :\r\n",
        "\r\n",
        "![Images marquées dans un projet](./images/tagged-images.jpg)\r\n",
        "\r\n",
        "## Entraîner et tester un modèle\r\n",
        "\r\n",
        "Maintenant que vous avez étiqueté les images de votre projet, vous êtes prêt à entraîner un modèle.\r\n",
        "\r\n",
        "1. Dans le projet Vision personnalisée, cliquez sur **Entraîner** pour entraîner un modèle de détection d’objets à l’aide des images étiquetées. Sélectionnez l’option **Formation rapide**.\r\n",
        "2. Attendez la fin de la formation (cela peut prendre une dizaine de minutes), puis vérifiez les mesures de performance *Précision*, *Rappel*, et *mAP* ; elles mesurent la précision de prédiction du modèle de classification et doivent toutes être élevées.\r\n",
        "3. En haut à droite de la page, cliquez sur **Test rapide**, puis dans la case **URL image**, saisissez `https://aka.ms/apple-orange` et regardez la prédiction qui est générée. Fermez ensuite la fenêtre **Test rapide**.\r\n",
        "\r\n",
        "## Publier et consommer le modèle de détection d’objets\r\n",
        "\r\n",
        "Vous êtes maintenant prêt à publier votre modèle entraîné et à l’utiliser à partir d’une application cliente.\r\n",
        "\r\n",
        "1. En haut à gauche de la page **Performances**, cliquez sur **&#128504; Publier** pour publier le modèle entraîné avec les paramètres suivants :\r\n",
        "    - **Nom du modèle** : détecter-produire\r\n",
        "    - **Ressource de prédiction** : *Votre **ressource** prédiction* de vision personnalisée.\r\n",
        "\r\n",
        "### (!) Vérification \r\n",
        "Avez-vous utilisé le même nom de modèle : **détecter-produire** ? \r\n",
        "\r\n",
        "2. Après la publication, cliquez sur l’icône *Paramètres* (&#9881;) en haut à droite de la page **Performances** pour afficher les paramètres du projet. Ensuite, sous **Généralités** (à gauche), copiez **ID du projet**. Faites défiler la page vers le bas et collez-la dans la cellule de code située sous l’étape 5 en remplaçant **YOUR_PROJECT_ID**. \r\n",
        "\r\n",
        "> (*(si vous avez utilisé une ressource **Cognitive Services** au lieu de créer une ressource **Vision personnalisée** au début de cet exercice, vous pouvez copier sa clé et son point de terminaison à partir de la partie droite des paramètres du projet, les coller dans la cellule de code ci-dessous et l’exécuter pour voir les résultats. Sinon, continuez à suivre les étapes ci-dessous pour obtenir la clé et le point de terminaison de votre ressource de prédiction Vision personnalisée*).\r\n",
        "\r\n",
        "3. En haut à gauche de la page **Paramètres du projet**, cliquez sur l’icône Galerie de projets (&128065;) pour revenir à la page d’*Galerie de projets* (&#128065;) pour revenir à la page d’accueil du portail Vision personnalisée, où votre projet est maintenant répertorié.\r\n",
        "\r\n",
        "4. Sur la page d’accueil du portail Vision personnalisée, en haut à droite, cliquez sur l’icône *Paramètres* (&#9881;) pour afficher les paramètres de votre service Vision personnalisée. Ensuite, sous **Ressources**, développez votre ressource *Prédiction* (<u>pas</u> la ressource Formation) et copiez ses valeurs **Clé** et **Point de terminaison** dans la cellule de code sous l’étape 5, en remplaçant **YOUR_KEY** et **YOUR_ENDPOINT**.\r\n",
        "\r\n",
        "### (!) Vérification \r\n",
        "Si vous utilisez une ressource **Vision personnalisée**, avez-vous utilisé la ressource **Prédiction** (<u>pas</u> la ressource Formation) ?\r\n",
        "\r\n",
        "5. Exécutez la cellule de code ci-dessous en cliquant sur le bouton Exécuter la cellule <span>&#9655;</span> (en haut à gauche de la cellule) pour définir les variables sur vos valeurs d’ID de projet, de clé et de point de terminaison."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\r\n",
        "cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\r\n",
        "cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\r\n",
        "\r\n",
        "model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\r\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692485387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez maintenant utiliser votre clé et votre point de terminaison avec un client Vision personnalisée pour vous connecter à votre modèle de détection d’objets de vision personnalisée.\r\n",
        "\r\n",
        "Exécutez la cellule de code suivante, qui utilise votre modèle pour détecter des produits individuels dans une image.\r\n",
        "\r\n",
        "> **Remarque** : Ne vous souciez pas des détails du code. Il utilise le SDK Python pour le service Vision personnalisée pour soumettre une image à votre modèle et récupérer les prédictions pour les objets détectés. Chaque prédiction se compose d’un nom de classe (*pomme*, *banane*, ou *orange*) et les coordonnées *case de délimitation* qui indiquent où, dans l’image, l’objet prédit a été détecté. Le code utilise ensuite ces informations pour dessiner une boîte étiquetée autour de chaque objet sur l’image."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\r\n",
        "from msrest.authentication import ApiKeyCredentials\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from PIL import Image, ImageDraw, ImageFont\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Load a test image and get its dimensions\r\n",
        "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\r\n",
        "test_img = Image.open(test_img_file)\r\n",
        "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\r\n",
        "\r\n",
        "# Get a prediction client for the object detection model\r\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\r\n",
        "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\r\n",
        "\r\n",
        "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\r\n",
        "\r\n",
        "# Detect objects in the test image\r\n",
        "with open(test_img_file, mode=\"rb\") as test_data:\r\n",
        "    results = predictor.detect_image(project_id, model_name, test_data)\r\n",
        "\r\n",
        "# Create a figure to display the results\r\n",
        "fig = plt.figure(figsize=(8, 8))\r\n",
        "plt.axis('off')\r\n",
        "\r\n",
        "# Display the image with boxes around each detected object\r\n",
        "draw = ImageDraw.Draw(test_img)\r\n",
        "lineWidth = int(np.array(test_img).shape[1]/100)\r\n",
        "object_colors = {\r\n",
        "    \"apple\": \"lightgreen\",\r\n",
        "    \"banana\": \"yellow\",\r\n",
        "    \"orange\": \"orange\"\r\n",
        "}\r\n",
        "for prediction in results.predictions:\r\n",
        "    color = 'white' # default for 'other' object tags\r\n",
        "    if (prediction.probability*100) > 50:\r\n",
        "        if prediction.tag_name in object_colors:\r\n",
        "            color = object_colors[prediction.tag_name]\r\n",
        "        left = prediction.bounding_box.left * test_img_w \r\n",
        "        top = prediction.bounding_box.top * test_img_h \r\n",
        "        height = prediction.bounding_box.height * test_img_h\r\n",
        "        width =  prediction.bounding_box.width * test_img_w\r\n",
        "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\r\n",
        "        draw.line(points, fill=color, width=lineWidth)\r\n",
        "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\r\n",
        "plt.imshow(test_img)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692585672
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher les prédictions résultantes, qui montrent les objets détectés et la probabilité de chaque prédiction."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}